{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import gensim\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "import warnings\n",
    "from keras import backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv', index_col='id', engine='python')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv', index_col='id', engine='python')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train and test shape: {} {}\".format(train_df.shape, test_df.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Target Feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.title(\"Target value Distributions\")\n",
    "sns.distplot(train_df['target'], kde=True, hist=False, bins=240, label='target')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see that most of the comments present in the dataset are actually non-toxic (<0.5) and only a few of them are actually toxic (>0.5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If toxicity rating < 0.5 then the comment is non-toxic else it is toxic.\n",
    "# Get toxic and non-toxic comments.\n",
    "temp = train_df['target'].apply(lambda x: \"non-toxic\" if x < 0.5 else \"toxic\")\n",
    "\n",
    "# Plot the number and percentage of toxic and non-toxic comments.\n",
    "fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "total = float(len(temp))\n",
    "\n",
    "# Plot the count plot.\n",
    "cntplot = sns.countplot(temp)\n",
    "cntplot.set_title('Percentage of non-toxic and toxic comments')\n",
    "\n",
    "# Get the height and calculate percentage then display it the plot itself.\n",
    "for p in ax.patches:\n",
    "    # Get height.\n",
    "    height = p.get_height()\n",
    "    # Plot at appropriate position.\n",
    "    ax.text(p.get_x() + p.get_width()/2.0, height + 3, '{:1.2f}%'.format(100*height/total), ha='center')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset is imbalanced as 92% of the comments are non-toxic and only 8% are toxic**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Toxicity Subtype Features:\n",
    "<ul>\n",
    "<li>severe_toxicity</li>\n",
    "<li>obscene</li>\n",
    "<li>threat</li>\n",
    "<li>insult</li>\n",
    "<li>identity_attack</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_distribution(features, title, data):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.title(title)\n",
    "    for feature in features:\n",
    "        sns.distplot(data[feature],kde=True,hist=False, bins=240, label=feature)\n",
    "    plt.xlabel('')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
    "plot_features_distribution(features, \"Distribution of additional toxicity features in the train set\", train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the distribution of additional toxicity features on the comments that are actually considered toxic:\n",
    "temp = train_df[train_df['target'] > 0.5]\n",
    "plot_features_distribution(features, \"Distribution of additional toxicity features in only toxic comments data\", temp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see that for toxic comments data, there are more insulting comments as compared to obscene comments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the count of additonal toxicity features in toxic comments data(temp):\n",
    "def get_comment_nature(row):\n",
    "    # Extract type of toxic comment\n",
    "    row = [row['severe_toxicity'], row['obscene'], row['identity_attack'], row['insult'], row['threat']]\n",
    "    \n",
    "    maxarg = np.argmax(np.array(row)) # Get the max value index.\n",
    "    \n",
    "    if maxarg == 0: return 'severe_toxicity'\n",
    "    elif maxarg == 1: return 'obscene'\n",
    "    elif maxarg == 2: return 'identity_attack'\n",
    "    elif maxarg == 3: return 'insult'\n",
    "    else: return 'threat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = temp.apply(get_comment_nature, axis=1) # Get nature of each toxic comment\n",
    "fig, ax = plt.subplots(1,1,figsize=(7,7))\n",
    "total = float(len(x))\n",
    "\n",
    "# Plot the count plot.\n",
    "cntplot = sns.countplot(x)\n",
    "cntplot.set_title('Percentage of toxicity nature in toxic comments data')\n",
    "\n",
    "# Get the height and calculate percentage then display it the plot itself.\n",
    "for p in ax.patches:\n",
    "    # Get height.\n",
    "    height = p.get_height()\n",
    "    # Plot at appropriate position.\n",
    "    ax.text(p.get_x() + p.get_width()/2.0, height + 3, '{:1.2f}%'.format(100*height/total), ha='center')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In our train dataset only 8% of the data was toxic. Out of that 8%, 81% of the toxic comments made are insults, 8.37% are identity attacks, 7.20% are obscene, 3.35% are threats and a very small amount of toxic comments are severly toxic.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identity Attributes:\n",
    "\n",
    "Sensitive topics:\n",
    "\n",
    "- male\n",
    "- female\n",
    "- homosexual_gay_or_lesbian\n",
    "- bisexual\n",
    "- heterosexual\n",
    "- christian\n",
    "- jewish\n",
    "- muslim\n",
    "- black\n",
    "- white\n",
    "- asian\n",
    "- latino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train_df.dropna(axis = 0, how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['male', 'female', 'transgender', 'other_gender']\n",
    "plot_features_distribution(features, \"Distribution of gender feature values\", temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['bisexual', 'heterosexual', 'homosexual_gay_or_lesbian', 'other_sexual_orientation']\n",
    "plot_features_distribution(features, \"Distribution of sexual orientation features values in the train set\", temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['asian', 'black', 'jewish', 'latino', 'other_race_or_ethnicity', 'white']\n",
    "plot_features_distribution(features, \"Distribution of race and ethnicity features values in the train set\", temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data where race/ethnic references are made.\n",
    "cond = (train_df['asian'] > 0.5) | (train_df['black'] > 0.5) | (train_df['jewish'] > 0.5) | (train_df['latino'] > 0.5) | (train_df['white'] > 0.5)\n",
    "temp = train_df[cond] # Get data where race/ethnic references are made.\n",
    "temp = temp[temp['target'] > 0.5] # Extract only toxic comments.\n",
    "\n",
    "x = temp.apply(get_comment_nature, axis=1) # Get nature of each toxic comment\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(7,7))\n",
    "total = float(len(x))\n",
    "\n",
    "# Plot the count plot.\n",
    "cntplot = sns.countplot(x)\n",
    "cntplot.set_title('Percentage of type of toxicity in comments where race/ethnic references are made')\n",
    "\n",
    "# Get the height and calculate percentage then display it the plot itself.\n",
    "for p in ax.patches:\n",
    "    # Get height.\n",
    "    height = p.get_height()\n",
    "    # Plot at appropriate position.\n",
    "    ax.text(p.get_x() + p.get_width()/2.0, height + 3, '{:1.2f}%'.format(100*height/total), ha='center')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see that the toxic comments involving words like black, asian etc. are mainly used for identity attacks or insults.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data where race/ethnic references are made.\n",
    "cond = (train_df['bisexual'] > 0.5) | (train_df['heterosexual'] > 0.5) | (train_df['homosexual_gay_or_lesbian'] > 0.5) | (train_df['other_sexual_orientation'] > 0.5) \n",
    "temp = train_df[cond] # Get data where race/ethnic references are made.\n",
    "temp = temp[temp['target'] > 0.5] # Extract only toxic comments.\n",
    "\n",
    "x = temp.apply(get_comment_nature, axis=1) # Get nature of each toxic comment\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(7,7))\n",
    "total = float(len(x))\n",
    "\n",
    "# Plot the count plot.\n",
    "cntplot = sns.countplot(x)\n",
    "cntplot.set_title('Percentage of type of toxicity in comments where sexual orientation references are made')\n",
    "\n",
    "# Get the height and calculate percentage then display it the plot itself.\n",
    "for p in ax.patches:\n",
    "    # Get height.\n",
    "    height = p.get_height()\n",
    "    # Plot at appropriate position.\n",
    "    ax.text(p.get_x() + p.get_width()/2.0, height + 3, '{:1.2f}%'.format(100*height/total), ha='center')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see from the plot that the toxic comments where sexual orientation references are made are mostly used for identity attacks.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data where race/ethnic references are made.\n",
    "cond = (train_df['male'] > 0.5) | (train_df['female'] > 0.5) | (train_df['transgender'] > 0.5) | (train_df['other_gender'] > 0.5) \n",
    "temp = train_df[cond] # Get data where race/ethnic references are made.\n",
    "temp = temp[temp['target'] > 0.5] # Extract only toxic comments.\n",
    "\n",
    "x = temp.apply(get_comment_nature, axis=1) # Get nature of each toxic comment\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(7,7))\n",
    "total = float(len(x))\n",
    "\n",
    "# Plot the count plot.\n",
    "cntplot = sns.countplot(x)\n",
    "cntplot.set_title('Percentage of type of toxicity in comments where gender references are made')\n",
    "\n",
    "# Get the height and calculate percentage then display it the plot itself.\n",
    "for p in ax.patches:\n",
    "    # Get height.\n",
    "    height = p.get_height()\n",
    "    # Plot at appropriate position.\n",
    "    ax.text(p.get_x() + p.get_width()/2.0, height + 3, '{:1.2f}%'.format(100*height/total), ha='center')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the plot we see that the toxic comments which involve words like male, female etc are insults.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Features generated by users feedback:\n",
    "\n",
    "- funny\n",
    "- sad\n",
    "- wow\n",
    "- likes\n",
    "- disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count(feature, title, data, size=1):\n",
    "    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n",
    "    total = float(len(data))\n",
    "    g = sns.countplot(data[feature], order = data[feature].value_counts().index[:20], palette='Set3')\n",
    "    g.set_title(\"Number and percentage of {}\".format(title))\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 3,\n",
    "                '{:1.2f}%'.format(100*height/total),\n",
    "                ha=\"center\") \n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count('funny','funny votes given', train_df, 3)\n",
    "plot_count('funny', 'funny votes given on toxic comments only', train_df[train_df['target'] > 0.5], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count('sad','sad votes given', train_df, 3)\n",
    "plot_count('sad', 'sad votes given on toxic comments only', train_df[train_df['target'] > 0.5], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count('wow','wow votes given', train_df, 3)\n",
    "plot_count('wow', 'wow votes given on toxic comments only', train_df[train_df['target'] > 0.5], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count('likes','likes given', train_df, 3)\n",
    "plot_count('likes', 'likes given on toxic comments only', train_df[train_df['target'] > 0.5], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count('disagree','disagree given', train_df, 3)\n",
    "plot_count('disagree', 'disagree given on toxic comments only', train_df[train_df['target'] > 0.5], 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comments_text Feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stpwrds = set(STOPWORDS)\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stpwrds,\n",
    "        max_words=50,\n",
    "        max_font_size=40, \n",
    "        scale=5,\n",
    "        random_state=1\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(10,10))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(train_df['comment_text'].sample(20000), title = 'Prevalent words in comments - train data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(train_df.loc[train_df['insult'] > 0.75]['comment_text'].sample(20000), \n",
    "               title = 'Prevalent comments with insult score > 0.75')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(train_df.loc[train_df['threat'] > 0.75]['comment_text'], \n",
    "               title = 'Prevalent words in comments with threat score > 0.75')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(train_df.loc[train_df['obscene'] > 0.75]['comment_text'], \n",
    "               title = 'Prevalent words in comments with obscene score > 0.75')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(train_df.loc[train_df['target'] > 0.75]['comment_text'], \n",
    "               title = 'Prevalent words in comments with target score > 0.75')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(train_df.loc[train_df['target'] < 0.25]['comment_text'], \n",
    "               title = 'Prevalent words in comments with target score < 0.25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(train_df.loc[train_df['obscene']< 0.25]['comment_text'], \n",
    "               title = 'Prevalent words in comments with obscene score < 0.25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(train_df.loc[train_df['threat'] < 0.25]['comment_text'], \n",
    "               title = 'Prevalent words in comments with threat score < 0.25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(train_df.loc[train_df['insult'] < 0.25]['comment_text'].sample(20000), \n",
    "               title = 'Prevalent comments with insult score < 0.25')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
